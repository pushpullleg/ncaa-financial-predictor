# üî¥ CRITICAL ISSUES: Professor's Feedback on ML Proposal

**Date:** November 16, 2025  
**Project:** NCAA EADA Athletic Department Financial Efficiency Prediction  
**Status:** REQUIRES MAJOR REVISION

---

## EXECUTIVE SUMMARY

**Current Proposal Status: ‚ùå FAILS TO MEET ML CRITERIA**

The analysis demonstrates good EDA skills but **fundamentally misunderstands what constitutes a machine learning problem**. The current approach attempts to predict a calculated metric (Revenue/Expenses) using the same data that defines it - this is circular reasoning, not prediction.

---

## CRITICAL ISSUE #1: Not a Prediction Problem (CIRCULAR REASONING)

### The Problem
```
Current Approach:
- Target: Efficiency_Ratio = Revenue / Expenses
- Features: Include Revenue, Expenses, Revenue_per_Athlete, Expense_per_Athlete

This is arithmetic, not machine learning!
```

### Why This Fails
- **Circular Logic:** Using Revenue and Expenses to predict Revenue/Expenses is deterministic
- **No Prediction:** You already have the answer - it's just division
- **No Temporal Gap:** Same-year data means no forecasting

### Professor's Question
> "If I give you Revenue and Expenses, why do I need a machine learning model to divide them? 
> That's a calculator's job, not ML."

### What ML Actually Requires
- Predict **FUTURE** outcomes (year t+1) using **CURRENT** data (year t)
- Predict **UNKNOWN** values using **KNOWN** features
- Create a **temporal gap** between input and output

---

## CRITICAL ISSUE #2: Data Quality Red Flags (MANIPULATED DATA)

### The Smoking Gun
```
Distribution Analysis:
- Surplus (>1.0):     6,905 (40.1%)
- Exactly 1.0:        10,315 (59.9%)  ‚Üê 60% EXACTLY 1.0
- Deficit (<1.0):     0 (0.0%)        ‚Üê ZERO DEFICITS
```

### Why This is Impossible
1. **60% report EXACTLY 1.0** - Not "around 1.0", literally 1.000000
   - This is accounting manipulation
   - Institutions use "institutional support" to artificially balance books
   - Real-world variation would show a distribution, not discrete values

2. **ZERO institutions report deficits** - Unrealistic
   - Athletic departments DO run deficits in reality
   - NCAA's own research shows 80%+ of programs lose money
   - Data shows 0% deficit = **financial reporting manipulation**

3. **This is not natural variation** - It's cookbook accounting
   - Institutions adjust "institutional support" to force efficiency = 1.0
   - Data reflects accounting rules, not actual financial performance

### Professor's Critique
> "Your data shows that 60% of institutions report exactly 1.0 efficiency. This isn't variation - 
> it's evidence of systematic reporting manipulation. You can't build predictive models on 
> fabricated data. What are you actually predicting? Who cooked their books better?"

---

## CRITICAL ISSUE #3: Extremely Weak Feature Correlations

### The Evidence
```
Top Features Correlated with Efficiency:
- Aid_per_Athlete:             -0.050  ‚Üê R¬≤ = 0.0025 (0.25% variance explained)
- Revenue_per_Athlete:         +0.049  ‚Üê R¬≤ = 0.0024 (0.24% variance explained)
- Men_Expense_Share:           -0.042  ‚Üê R¬≤ = 0.0018 (0.18% variance explained)
- Total_Recruiting_Expenses:   +0.035  ‚Üê R¬≤ = 0.0012 (0.12% variance explained)
- Women_Share:                 -0.034  ‚Üê R¬≤ = 0.0012 (0.12% variance explained)
```

### Why This Fails ML Criteria
- **Correlations < 0.1 are statistically meaningless** for prediction
- Combined, these features explain **less than 1% of variance**
- **No signal to learn from** - ML models will perform no better than random guessing
- This actually **proves ML is inappropriate**, not that it's justified

### Professor's Critique
> "You claim these weak correlations justify machine learning. In fact, they prove the opposite.
> Correlations below 0.1 indicate there's NO relationship to model. Your features have almost 
> zero predictive power."

### What Strong Correlations Look Like
- **Weak:** < 0.3 (not useful for prediction)
- **Moderate:** 0.3 - 0.7 (suitable for ML)
- **Strong:** > 0.7 (excellent for prediction)
- **Your data:** < 0.05 (no signal)

---

## CRITICAL ISSUE #4: Misapplication of Hurdle Models

### Your Proposal
> "Two-stage hurdle model to handle zero-inflation"

### The Misunderstanding
- **Hurdle models** are for count data with **excess zeros** (e.g., number of citations, hospital visits)
- Used when outcome is 0 for one process, > 0 for another process
- Example: Number of doctor visits (many people = 0 visits, some people = 1, 2, 3...)

### Your Data
- **Not count data** - Efficiency is a continuous ratio
- **Not excess zeros** - You have excess ONES (1.0), not zeros
- **Not two processes** - All institutions follow same accounting rules

### Why Your Approach is Wrong
- Efficiency = 1.0 is **artificial constraint**, not a natural zero
- The 60% at 1.0 represents **data manipulation**, not a statistical phenomenon
- Hurdle models don't apply to censored continuous data

### Professor's Critique
> "Zero-inflation refers to count data with structural zeros. Your data has 60% reporting 
> exactly 1.0 due to accounting manipulation. This is censored data, not zero-inflated. 
> You're applying the wrong statistical framework entirely."

---

## CRITICAL ISSUE #5: Zero Temporal Variance (SELF-CONTRADICTION)

### Your Claim
```
6. TEMPORAL PATTERNS
   ‚Ä¢ Efficiency changes over time (variance: 0.0000)
   ‚Ä¢ Suggests temporal features can improve predictions
```

### The Contradiction
- **Variance = 0.0000** means efficiency is **constant over time**
- If variance is literally zero, temporal features add **no information**
- You claim time matters, then show it doesn't - **self-contradictory**

### What This Actually Means
- Institutions maintain constant reported efficiency year-over-year
- They adjust "institutional support" to hit the same target each year
- **No temporal signal to learn from**

### Professor's Critique
> "You state that temporal variance is 0.0000, then immediately claim temporal features will 
> improve predictions. If there's no variance over time, time has no predictive power. 
> This is logically inconsistent."

---

## CRITICAL ISSUE #6: Distribution Shift (Train vs Test)

### The Evidence
```
Training Set (2013-2020): Surplus rate = 40.7%
Validation Set (2021-2022): Surplus rate = 39.1%
Test Set (2023):          Surplus rate = 37.9%
```

### The Problem
- **Declining trend:** Surplus rate drops from 40.7% ‚Üí 37.9%
- **Distribution shift:** Test set is NOT representative of training data
- **Model will fail:** Trained on 40.7% surplus, tested on 37.9% surplus

### Why This Matters
- Model learns "40% of institutions achieve surplus"
- Reality in 2023: Only 38% achieve surplus
- Model will be **overconfident** and **miscalibrated**

### Professor's Critique
> "Your test set has a different class distribution than your training set. This is distribution 
> shift, and it means your model's performance metrics will be overly optimistic. In production, 
> the model will underperform."

---

## CRITICAL ISSUE #7: Missing Baseline Comparison

### What's Missing
You never established what a **naive baseline** would achieve:

**Naive Baseline Strategy:**
```python
# Always predict "No Surplus" (efficiency = 1.0)
# Since 60% report exactly 1.0, baseline accuracy = 60%
```

### Why This is Critical
- ML is only justified if it **beats the baseline by meaningful margin** (5-10%+)
- If XGBoost achieves 65% accuracy, that's only 5% improvement over naive guess
- **Not worth the complexity** of ML for marginal gains

### Professor's Critique
> "You haven't shown that machine learning adds value. What's the baseline? If I just predict 
> every institution reports 1.0 efficiency, I'm right 60% of the time. Can your ML model beat 
> that by enough to justify the complexity?"

---

## CRITICAL ISSUE #8: No Clear Business Value

### Your Claim
> "Predict institutions at risk of financial deficit"

### The Reality
- **Zero institutions report deficits** in your data
- How can you predict something that never happens?
- Risk assessment requires variation in the outcome

### What You Should Be Predicting
1. **Next year's revenue/expenses** - Actionable for budgeting
2. **Which institutions manipulate reporting** - Valuable for compliance
3. **Future surplus probability** - Useful for strategic planning
4. **Division reclassification impact** - Supports policy decisions

### Professor's Critique
> "You claim to predict institutions at risk of deficit, but your data shows zero deficits. 
> This is contradictory. What exactly is the business value of predicting who will report 
> exactly 1.0 vs. slightly above 1.0?"

---

## WHAT NEEDS TO BE FIXED: Action Plan

### 1. Redefine the Prediction Problem

**‚ùå WRONG (Current):**
```python
# Predict 2023 efficiency using 2023 data
X_2023 = [Revenue_2023, Expenses_2023, ...]
y_2023 = Revenue_2023 / Expenses_2023
```

**‚úÖ CORRECT (Revised):**
```python
# Predict 2023 surplus using 2022 data
X_2022 = [Enrollment_2022, Athletes_2022, Division, State, Lag_Revenue_2021, ...]
y_2023 = Has_Surplus_2023  # Binary: 1 if efficiency > 1.0, else 0
```

### 2. Create Proper Temporal Features

**Required transformations:**
```python
# Lag features (prior year values)
df['Lag1_Revenue'] = df.groupby('UNITID')['Grand Total Revenue'].shift(1)
df['Lag1_Expenses'] = df.groupby('UNITID')['Grand Total Expenses'].shift(1)
df['Lag1_Efficiency'] = df.groupby('UNITID')['Efficiency_Ratio'].shift(1)

# Target (next year outcome)
df['Target_Next_Year_Surplus'] = df.groupby('UNITID')['Has_Surplus'].shift(-1)

# Growth rates
df['Revenue_Growth'] = df['Grand Total Revenue'].pct_change()
df['Expense_Growth'] = df['Grand Total Expenses'].pct_change()
```

### 3. Remove Current-Year Financial Data from Features

**DO NOT USE as features:**
- ‚ùå Grand Total Revenue (current year)
- ‚ùå Grand Total Expenses (current year)
- ‚ùå Revenue_per_Athlete (derived from current revenue)
- ‚ùå Expense_per_Athlete (derived from current expenses)
- ‚ùå Efficiency_Ratio (current year) - this is the target!

**USE INSTEAD:**
- ‚úÖ Lag1_Revenue (prior year)
- ‚úÖ Lag1_Expenses (prior year)
- ‚úÖ Revenue_Growth (YoY change)
- ‚úÖ Division, State, Enrollment (structural features)
- ‚úÖ Total_Athletes, Women_Share (participation metrics)

### 4. Establish Baseline Performance

**Baseline strategies to compare:**
```python
# Baseline 1: Always predict majority class
baseline_1 = accuracy_score(y_test, [0] * len(y_test))  # Always "No Surplus"

# Baseline 2: Predict based on prior year
baseline_2 = accuracy_score(y_test, X_test['Lag1_Has_Surplus'])  # Persistence

# Baseline 3: Division average
baseline_3 = predict_by_division_mean(X_test, y_train)

# ML must beat best baseline by 5-10% to be justified
```

### 5. Investigate the "Exactly 1.0" Phenomenon

**Two possible approaches:**

**Option A: Exclude artificial data**
```python
# Only analyze institutions that report real numbers
df_real = df[df['Efficiency_Ratio'] != 1.0]
# Build model on genuine variation
```

**Option B: Predict data manipulation**
```python
# New target: Does institution manipulate reporting?
df['Reports_Exactly_One'] = (df['Efficiency_Ratio'] == 1.0).astype(int)
# Predict which institutions cook the books
```

### 6. Recalculate Feature Correlations with Lag Features

**Expected improvement:**
```python
# Current (same-year) - WEAK
corr(Revenue_2023, Efficiency_2023) ‚âà 0.05

# Fixed (prior-year) - STRONGER
corr(Revenue_2022, Surplus_2023) ‚âà 0.3-0.5
```

### 7. Show ML Improvement Over Baseline

**Required evidence:**
- Baseline accuracy: ~60% (naive prediction)
- ML accuracy: 70-75% (10-15 point improvement)
- ROC-AUC: 0.75-0.80
- **Improvement must justify complexity**

---

## REVISED PROBLEM STATEMENT

### ‚ùå ORIGINAL (FAILED)
> "Predict athletic department efficiency ratio using current-year financial and participation data"

### ‚úÖ REVISED (CORRECT)
> "**Forecast whether an athletic department will achieve financial surplus in the upcoming year**, 
> using only current-year institutional characteristics (Division, enrollment, participation) and 
> lagged financial indicators (prior year revenue/expenses), to support strategic budget planning 
> and risk assessment."

---

## REVISED RESEARCH QUESTIONS

### Primary Question
**Can we predict next year's financial surplus status using current institutional characteristics and historical financial performance?**

### Secondary Questions
1. Which institutional factors (Division, enrollment, participation) most strongly predict future surplus?
2. How much does prior-year financial performance predict next-year outcomes?
3. Can ML models outperform simple heuristics (e.g., "same as last year")?
4. What is the predictive accuracy degradation from training to test periods?

---

## EVALUATION CRITERIA (What Your Professor Expects)

### Minimum Requirements to Pass

1. **‚úÖ Clear Prediction Target**
   - Must be **unknown at time of prediction**
   - Temporal gap between features (year t) and target (year t+1)
   - No circular reasoning (using target to predict itself)

2. **‚úÖ Feature Correlations > 0.2**
   - At least moderate relationships with target
   - Evidence that signal exists to learn from
   - Justify why ML > simple rules

3. **‚úÖ Baseline Comparison**
   - Establish naive baseline performance
   - Show ML improvement of 5-10%+
   - Prove complexity is justified

4. **‚úÖ Proper Train/Test Split**
   - No data leakage
   - Test set truly represents future prediction
   - Account for distribution shift

5. **‚úÖ Interpretability**
   - SHAP values show meaningful patterns
   - Insights are actionable (not just "Revenue matters")
   - Business value is clear

### Excellence Criteria (A-grade work)

6. **‚úÖ Address Data Quality Issues**
   - Investigate the 60% exactly 1.0 phenomenon
   - Handle censored/manipulated data appropriately
   - Transparent about limitations

7. **‚úÖ Multiple Modeling Approaches**
   - Compare linear vs. tree-based models
   - Test different feature sets
   - Sensitivity analysis

8. **‚úÖ Real Business Impact**
   - Case study (e.g., TAMUC division change)
   - Recommendations for athletic directors
   - Policy implications

---

## EXPECTED OUTCOMES (Realistic Targets)

### Model Performance
- **Baseline accuracy:** 60% (predict all "No Surplus")
- **ML accuracy:** 70-75% (10-15 point improvement)
- **ROC-AUC:** 0.75-0.80
- **Precision/Recall:** 0.70/0.65 (balanced)

### Key Findings (Hypothesized)
1. **Division is strongest predictor** (D1 > D2 > D3 for surplus probability)
2. **Prior-year efficiency predicts next year** (persistence ~0.6 correlation)
3. **Enrollment matters** (larger institutions more likely surplus)
4. **Gender balance weakly correlates** (higher female % ‚Üí higher surplus)

### Business Value
1. **Budget forecasting:** Predict next year's financial position
2. **Risk identification:** Flag institutions likely to struggle
3. **Division decisions:** Model impact of reclassification
4. **Resource allocation:** Optimize spending patterns

---

## CONCLUSION

### Current Status
**The analysis demonstrates good technical execution (coding, visualization) but fundamentally 
misunderstands the machine learning problem. The core issue is circular reasoning: attempting 
to predict a calculated metric using the data that defines it.**

### Path Forward
**Complete revision required. Must reframe as a genuine forecasting problem with temporal 
separation between input features (year t) and target outcome (year t+1). Only then can 
this qualify as machine learning rather than arithmetic.**

### Professor's Final Word
> "You have good data and coding skills. But machine learning isn't about applying XGBoost 
> to everything - it's about solving prediction problems that can't be solved with simple 
> rules or arithmetic. Your current approach fails this test. Revise with a proper temporal 
> forecasting framework, and we can discuss further."

---

## REFERENCES FOR REVISION

### Recommended Reading
1. **Time Series Forecasting:** "Forecasting: Principles and Practice" - Hyndman & Athanasopoulos
2. **Feature Engineering:** "Feature Engineering for Machine Learning" - Zheng & Casari
3. **Baseline Comparison:** "Machine Learning Yearning" - Andrew Ng
4. **Athletic Finances:** NCAA Financial Database Glossary (2019)

### Key Concepts to Master
- Temporal train/test splits
- Lag features / autoregressive models
- Distribution shift / concept drift
- Baseline model comparison
- Censored data handling

---

**Document Version:** 1.0  
**Last Updated:** November 16, 2025  
**Status:** REQUIRES IMMEDIATE ATTENTION
